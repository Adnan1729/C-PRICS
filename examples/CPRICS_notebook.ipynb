{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1dfc112-5c50-4d30-88c0-1bc6936bef28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.integrate import simpson\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a66bde-e04d-4624-a555-76db13b80623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define input and output directories\n",
    "input_dir = '/Workspace/Users/amahmud1@networkrail.co.uk/CT/CT_raw_data/'\n",
    "output_dir = '/Workspace/Users/amahmud1@networkrail.co.uk/CT/CT_processed_data/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Configuration variables\n",
    "peak_amp = 3\n",
    "cols_to_use = [\n",
    "    'CT_Index', 'ELR', 'TrackId', '45_top_right', '45_top_left', '6_top_right',\n",
    "    '6_top_left', '9_top_right', '9_top_left', '135_top_right', '135_top_left', \n",
    "    '18_top_right', '18_top_left', 'Location_Norm_m'\n",
    "]\n",
    "\n",
    "ct_channels = [\n",
    "    '45_top_right', '45_top_left', '6_top_right', '6_top_left',\n",
    "    '9_top_right', '9_top_left', '135_top_right', '135_top_left',\n",
    "    '18_top_right', '18_top_left'\n",
    "]\n",
    "\n",
    "def sigmoid_value(val):\n",
    "    \"\"\"Placeholder function for sigmoid value calculation\"\"\"\n",
    "    return 0.0\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename format 'OWW-2100-134404-222214-2021-08-12.csv'\"\"\"\n",
    "    try:\n",
    "        parts = filename.replace('.csv', '').split('-')\n",
    "        year = parts[-3]\n",
    "        month = parts[-2]\n",
    "        day = parts[-1]\n",
    "        date_str = f\"{year}-{month}-{day}\"\n",
    "        datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        return date_str\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract valid date from filename: {filename}. Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_peak_islands(df, col):\n",
    "    \"\"\"Find islands of consecutive peaks and their triplet scores\"\"\"\n",
    "    # Create a boolean mask for peaks\n",
    "    peaks = df[col] >= peak_amp\n",
    "    \n",
    "    # Create groups of consecutive peaks\n",
    "    peak_groups = (peaks != peaks.shift()).cumsum()[peaks]\n",
    "    \n",
    "    peak_data = []\n",
    "    island_counter = 1  # Counter for island IDs\n",
    "    \n",
    "    for group_id in peak_groups.unique():\n",
    "        # Get indices for this group of peaks\n",
    "        group_indices = peak_groups[peak_groups == group_id].index\n",
    "        \n",
    "        # Only process groups with 3 or more consecutive peaks\n",
    "        if len(group_indices) >= 3:\n",
    "            group_df = df.loc[group_indices]\n",
    "            \n",
    "            # Create all possible triplets within this group\n",
    "            for i in range(len(group_df) - 2):\n",
    "                triplet_indices = group_df.index[i:i+3]\n",
    "                triplet_data = group_df.loc[triplet_indices]\n",
    "                \n",
    "                triplet_score = sum(triplet_data[col].values + \n",
    "                                  [sigmoid_value(v) for v in triplet_data[col].values])\n",
    "                \n",
    "                # Store data for each point in the triplet\n",
    "                for point in triplet_indices:\n",
    "                    peak_data.append({\n",
    "                        'CT_Index': df.loc[point, 'CT_Index'],\n",
    "                        'Location_Norm_m': df.loc[point, 'Location_Norm_m'],\n",
    "                        'Value': df.loc[point, col],\n",
    "                        'triplet_score': triplet_score,\n",
    "                        'island_id': island_counter,\n",
    "                        'island_size': len(group_indices)\n",
    "                    })\n",
    "            \n",
    "            island_counter += 1  # Increment island ID for next group\n",
    "    \n",
    "    if not peak_data:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    peaks_df = pd.DataFrame(peak_data)\n",
    "    \n",
    "    # For each CT_Index, keep only the highest triplet score\n",
    "    peaks_df = peaks_df.loc[peaks_df.groupby('CT_Index')['triplet_score'].idxmax()]\n",
    "    \n",
    "    return peaks_df\n",
    "\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"Process a single CT data file and return results\"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        ct = pd.read_csv(file_path, usecols=cols_to_use)\n",
    "        \n",
    "        final_results = []\n",
    "        \n",
    "        # Process each channel\n",
    "        for ct_channel in ct_channels:\n",
    "            # Get data for this channel\n",
    "            ct_channel_df = ct[['CT_Index', 'Location_Norm_m', ct_channel]].dropna(subset=[ct_channel])\n",
    "            \n",
    "            # Find peaks and their scores\n",
    "            peaks_df = find_peak_islands(ct_channel_df, ct_channel)\n",
    "            \n",
    "            if not peaks_df.empty:\n",
    "                # Add channel name\n",
    "                peaks_df['Channel'] = ct_channel\n",
    "                final_results.append(peaks_df)\n",
    "        \n",
    "        if final_results:\n",
    "            # Combine all results\n",
    "            return pd.concat(final_results, ignore_index=True)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process all files\"\"\"\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.endswith('.csv'):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing file: {filename}\")\n",
    "        \n",
    "        # Extract date from filename\n",
    "        date = extract_date_from_filename(filename)\n",
    "        if not date:\n",
    "            continue\n",
    "            \n",
    "        # Process the file\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        results_df = process_single_file(file_path)\n",
    "        \n",
    "        if not results_df.empty:\n",
    "            # Add date column\n",
    "            results_df['Date'] = date\n",
    "            \n",
    "            # Save to output file\n",
    "            output_file = os.path.join(output_dir, f\"{date}.csv\")\n",
    "            results_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved results to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a111866d-102f-42f7-bf86-69463c594782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CTFeatureExtractor:\n",
    "    def __init__(self, data, location_col='Location_Norm_m', value_col='Value', \n",
    "                 island_id_col='island_id', interpolation_points=100):\n",
    "        self.data = data\n",
    "        self.location_col = location_col\n",
    "        self.value_col = value_col\n",
    "        self.island_id_col = island_id_col\n",
    "        self.interpolation_points = interpolation_points\n",
    "        self.features = {}\n",
    "\n",
    "    def fit_spline(self, x, y):\n",
    "        \"\"\"Fit cubic spline to the data\"\"\"\n",
    "        # Use keyword arguments for CubicSpline\n",
    "        cs = CubicSpline(x=x, y=y)\n",
    "        x_new = np.linspace(start=x.min(), stop=x.max(), num=self.interpolation_points)\n",
    "        y_new = cs(x=x_new)\n",
    "        return x_new, y_new, cs\n",
    "\n",
    "    def calculate_features(self, island_id):\n",
    "        \"\"\"Calculate features for a specific island\"\"\"\n",
    "        # Get data for this island\n",
    "        island_data = self.data[self.data[self.island_id_col] == island_id].sort_values(self.location_col)\n",
    "        x = island_data[self.location_col].values\n",
    "        y = island_data[self.value_col].values\n",
    "        \n",
    "        # Store start and end locations\n",
    "        start_location = x[0]\n",
    "        end_location = x[-1]\n",
    "        \n",
    "        # Fit spline\n",
    "        x_interp, y_interp, spline = self.fit_spline(x, y)\n",
    "        \n",
    "        # Calculate derivatives\n",
    "        dy_dx = spline.derivative()(x_interp)\n",
    "        \n",
    "        # Calculate features\n",
    "        features = {\n",
    "            'island_id': island_id,\n",
    "            'start_location': start_location,  # Add start location\n",
    "            'end_location': end_location,      # Add end location\n",
    "            'peak_amplitude': np.max(y_interp),\n",
    "            'average_y': np.mean(y_interp),\n",
    "            'pattern_width': x_interp[-1] - x_interp[0],\n",
    "            'center_location': x_interp[np.argmax(y_interp)],\n",
    "            'mean_slope': np.mean(np.abs(dy_dx)),\n",
    "            'max_slope': np.max(np.abs(dy_dx)),\n",
    "            'area_under_curve': simpson(y=y_interp, x=x_interp)\n",
    "        }\n",
    "        \n",
    "        # Calculate distance difference\n",
    "        scalar_distance = np.sqrt((x_interp[-1] - x_interp[0])**2 + \n",
    "                                (y_interp[-1] - y_interp[0])**2)\n",
    "        vector_distance = np.sum(np.sqrt(np.diff(x_interp)**2 + \n",
    "                                       np.diff(y_interp)**2))\n",
    "        features['distance_difference'] = vector_distance - scalar_distance\n",
    "        \n",
    "        # Store interpolated values for plotting\n",
    "        features['x_interp'] = x_interp\n",
    "        features['y_interp'] = y_interp\n",
    "        features['dy_dx'] = dy_dx\n",
    "        features['x_raw'] = x\n",
    "        features['y_raw'] = y\n",
    "        \n",
    "        return features\n",
    "           \n",
    "    def extract_all_features(self):\n",
    "        \"\"\"Extract features for all islands\"\"\"\n",
    "        for island_id in self.data[self.island_id_col].unique():\n",
    "            self.features[island_id] = self.calculate_features(island_id)\n",
    "        return self.features\n",
    "    \n",
    "    def plot_all_islands(self, date):\n",
    "      \"\"\"Create a single plot showing all islands for a given date\"\"\"\n",
    "      fig = go.Figure()\n",
    "      \n",
    "      # Color scale for different risk scores\n",
    "      colors = plt.cm.RdYlGn_r(np.linspace(0, 1, 100))  # Red (high risk) to Green (low risk)\n",
    "      \n",
    "      for island_id, features in self.features.items():\n",
    "          risk_score = features.get('risk_score', 0)\n",
    "          color_idx = int(risk_score * 99)  # Map 0-1 to 0-99\n",
    "          color = f'rgb({colors[color_idx][0]*255},{colors[color_idx][1]*255},{colors[color_idx][2]*255})'\n",
    "          \n",
    "          # Plot original points\n",
    "          fig.add_trace(\n",
    "              go.Scatter(\n",
    "                  x=features['x_raw'],\n",
    "                  y=features['y_raw'],\n",
    "                  mode='markers',\n",
    "                  name=f'Island {island_id} (Risk: {risk_score:.2f})',\n",
    "                  marker=dict(size=8, color=color)\n",
    "              )\n",
    "          )\n",
    "          \n",
    "          # Plot spline fit\n",
    "          fig.add_trace(\n",
    "              go.Scatter(\n",
    "                  x=features['x_interp'],\n",
    "                  y=features['y_interp'],\n",
    "                  mode='lines',\n",
    "                  name=f'Spline {island_id}',\n",
    "                  line=dict(color=color),\n",
    "                  showlegend=False\n",
    "              )\n",
    "          )\n",
    "      \n",
    "      fig.update_layout(\n",
    "          title=f'CT Signal Analysis - {date}',\n",
    "          xaxis_title='Location (m)',\n",
    "          yaxis_title='Signal Value',\n",
    "          showlegend=True,\n",
    "          height=600,\n",
    "          width=1200\n",
    "      )\n",
    "      \n",
    "      return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769d122d-ff3b-4c32-a286-984a747f36bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_sorted_files(input_dir):\n",
    "    \"\"\"Get list of CSV files sorted by date in filename\"\"\"\n",
    "    files = []\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Extract date from filename (assuming format YYYY-MM-DD.csv)\n",
    "            date_str = filename.replace('.csv', '')\n",
    "            try:\n",
    "                date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                files.append((date, filename))\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Couldn't parse date from filename: {filename}\")\n",
    "                continue\n",
    "    \n",
    "    # Sort files by date\n",
    "    files.sort(key=lambda x: x[0])\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71c2149-2183-4f46-b2ab-a3dfd86bd3e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class IslandIdentifier:\n",
    "    def __init__(self, distance_threshold=5.0, overlap_threshold=0.3):\n",
    "        \"\"\"\n",
    "        Initialize IslandIdentifier with configurable thresholds\n",
    "        \n",
    "        Parameters:\n",
    "        distance_threshold: Maximum distance (meters) between island boundaries for matching\n",
    "        overlap_threshold: Minimum overlap ratio required for potential splits/merges\n",
    "        \"\"\"\n",
    "        self.distance_threshold = distance_threshold\n",
    "        self.overlap_threshold = overlap_threshold\n",
    "        self.registered_islands = pd.DataFrame(columns=[\n",
    "            'global_id',\n",
    "            'start_location',\n",
    "            'end_location',\n",
    "            'first_date',\n",
    "            'last_date',\n",
    "            'status',  # 'active', 'merged', 'split', 'disappeared'\n",
    "            'related_islands',  # IDs of split/merged islands\n",
    "            'confidence'  # confidence score for location matching\n",
    "        ])\n",
    "        self.next_id = 1\n",
    "        self.history = []  # Track all changes to islands\n",
    "\n",
    "    def calculate_overlap(self, island1, island2):\n",
    "        \"\"\"Calculate overlap ratio between two islands\"\"\"\n",
    "        start = max(island1['start_location'], island2['start_location'])\n",
    "        end = min(island1['end_location'], island2['end_location'])\n",
    "        \n",
    "        if end <= start:\n",
    "            return 0.0\n",
    "            \n",
    "        overlap = end - start\n",
    "        length1 = island1['end_location'] - island1['start_location']\n",
    "        length2 = island2['end_location'] - island2['start_location']\n",
    "        \n",
    "        return overlap / min(length1, length2)\n",
    "\n",
    "    def find_matches(self, island, current_date):\n",
    "        \"\"\"\n",
    "        Find potential matches for an island\n",
    "        Returns matches with confidence scores\n",
    "        \"\"\"\n",
    "        active_islands = self.registered_islands[\n",
    "            self.registered_islands['status'] == 'active'\n",
    "        ]\n",
    "        \n",
    "        matches = []\n",
    "        for _, registered in active_islands.iterrows():\n",
    "            # Calculate different matching metrics\n",
    "            start_diff = abs(registered['start_location'] - island['start_location'])\n",
    "            end_diff = abs(registered['end_location'] - island['end_location'])\n",
    "            overlap = self.calculate_overlap(island, registered)\n",
    "            \n",
    "            # Check if it's a potential match\n",
    "            if (start_diff <= self.distance_threshold and \n",
    "                end_diff <= self.distance_threshold):\n",
    "                \n",
    "                # Calculate confidence score (0-1)\n",
    "                confidence = 1.0 - max(\n",
    "                    start_diff / self.distance_threshold,\n",
    "                    end_diff / self.distance_threshold\n",
    "                ) * (1 - overlap)\n",
    "                \n",
    "                matches.append({\n",
    "                    'global_id': registered['global_id'],\n",
    "                    'confidence': confidence,\n",
    "                    'overlap': overlap\n",
    "                })\n",
    "        \n",
    "        return sorted(matches, key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "    def register_new_island(self, island, date):\n",
    "        \"\"\"Register a new island and return its global ID\"\"\"\n",
    "        global_id = f\"IS_{self.next_id:04d}\"\n",
    "        self.next_id += 1\n",
    "        \n",
    "        new_island = pd.DataFrame({\n",
    "            'global_id': [global_id],\n",
    "            'start_location': [island['start_location']],\n",
    "            'end_location': [island['end_location']],\n",
    "            'first_date': [date],\n",
    "            'last_date': [date],\n",
    "            'status': ['active'],\n",
    "            'related_islands': [[]],\n",
    "            'confidence': [1.0]\n",
    "        })\n",
    "        \n",
    "        self.registered_islands = pd.concat([self.registered_islands, new_island], \n",
    "                                          ignore_index=True)\n",
    "        \n",
    "        self.history.append({\n",
    "            'date': date,\n",
    "            'event': 'new',\n",
    "            'global_id': global_id,\n",
    "            'location': f\"{island['start_location']:.1f}-{island['end_location']:.1f}m\"\n",
    "        })\n",
    "        \n",
    "        return global_id\n",
    "\n",
    "    def update_island(self, global_id, island, date, confidence):\n",
    "        \"\"\"Update an existing island's record\"\"\"\n",
    "        idx = self.registered_islands['global_id'] == global_id\n",
    "        self.registered_islands.loc[idx, 'last_date'] = date\n",
    "        self.registered_islands.loc[idx, 'confidence'] = min(\n",
    "            self.registered_islands.loc[idx, 'confidence'].iloc[0],\n",
    "            confidence\n",
    "        )\n",
    "\n",
    "    def check_splits_and_merges(self, current_islands, date):\n",
    "        \"\"\"Check for potential split or merged islands\"\"\"\n",
    "        active_islands = self.registered_islands[\n",
    "            self.registered_islands['status'] == 'active'\n",
    "        ]\n",
    "        \n",
    "        # Group current islands by proximity\n",
    "        grouped_current = []\n",
    "        for island in current_islands:\n",
    "            added = False\n",
    "            for group in grouped_current:\n",
    "                if any(self.calculate_overlap(island, existing) > self.overlap_threshold \n",
    "                      for existing in group):\n",
    "                    group.append(island)\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                grouped_current.append([island])\n",
    "        \n",
    "        # Check each group for splits/merges\n",
    "        for group in grouped_current:\n",
    "            if len(group) > 1:  # Potential split\n",
    "                matches = []\n",
    "                for island in group:\n",
    "                    matches.extend(self.find_matches(island, date))\n",
    "                \n",
    "                if len(set(m['global_id'] for m in matches)) == 1:\n",
    "                    # Split detected\n",
    "                    parent_id = matches[0]['global_id']\n",
    "                    self.mark_split(parent_id, group, date)\n",
    "            \n",
    "            elif len(group) == 1 and len(self.find_matches(group[0], date)) > 1:\n",
    "                # Potential merge\n",
    "                self.mark_merge(group[0], self.find_matches(group[0], date), date)\n",
    "\n",
    "    def mark_split(self, parent_id, new_islands, date):\n",
    "        \"\"\"Mark an island as split and create new islands\"\"\"\n",
    "        # Update parent island\n",
    "        idx = self.registered_islands['global_id'] == parent_id\n",
    "        self.registered_islands.loc[idx, 'status'] = 'split'\n",
    "        \n",
    "        # Create new islands\n",
    "        new_ids = []\n",
    "        for island in new_islands:\n",
    "            new_id = self.register_new_island(island, date)\n",
    "            new_ids.append(new_id)\n",
    "        \n",
    "        self.history.append({\n",
    "            'date': date,\n",
    "            'event': 'split',\n",
    "            'parent_id': parent_id,\n",
    "            'child_ids': new_ids\n",
    "        })\n",
    "\n",
    "    def mark_merge(self, new_island, parent_matches, date):\n",
    "        \"\"\"Mark islands as merged and create a new merged island\"\"\"\n",
    "        # Update parent islands\n",
    "        parent_ids = [m['global_id'] for m in parent_matches]\n",
    "        for parent_id in parent_ids:\n",
    "            idx = self.registered_islands['global_id'] == parent_id\n",
    "            self.registered_islands.loc[idx, 'status'] = 'merged'\n",
    "        \n",
    "        # Create new merged island\n",
    "        new_id = self.register_new_island(new_island, date)\n",
    "        \n",
    "        self.history.append({\n",
    "            'date': date,\n",
    "            'event': 'merge',\n",
    "            'parent_ids': parent_ids,\n",
    "            'new_id': new_id\n",
    "        })\n",
    "\n",
    "    def register_islands(self, df_date, date):\n",
    "        \"\"\"\n",
    "        Register islands from a specific date and assign consistent global IDs\n",
    "        \"\"\"\n",
    "        df = df_date.copy()\n",
    "        df['global_id'] = None\n",
    "        df['match_confidence'] = 1.0\n",
    "        \n",
    "        # Process each island in the current date\n",
    "        current_islands = df.to_dict('records')\n",
    "        \n",
    "        # Check for splits and merges\n",
    "        self.check_splits_and_merges(current_islands, date)\n",
    "        \n",
    "        # Process each island\n",
    "        for idx, island in df.iterrows():\n",
    "            matches = self.find_matches(island, date)\n",
    "            \n",
    "            if not matches:\n",
    "                # New island\n",
    "                global_id = self.register_new_island(island, date)\n",
    "                df.loc[idx, 'global_id'] = global_id\n",
    "                df.loc[idx, 'match_confidence'] = 1.0\n",
    "            else:\n",
    "                # Use best match\n",
    "                best_match = matches[0]\n",
    "                df.loc[idx, 'global_id'] = best_match['global_id']\n",
    "                df.loc[idx, 'match_confidence'] = best_match['confidence']\n",
    "                self.update_island(best_match['global_id'], island, date, \n",
    "                                 best_match['confidence'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_history_summary(self):\n",
    "        \"\"\"Return a summary of island history\"\"\"\n",
    "        return pd.DataFrame(self.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210b6b48-e612-40bb-9d59-0022036eaaac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CTAnalyzer:\n",
    "    def __init__(self, n_clusters=5):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.scaler = StandardScaler()\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.feature_cols = [\n",
    "            'peak_amplitude', 'average_y', 'distance_difference',\n",
    "            'pattern_width', 'area_under_curve', 'mean_slope', 'max_slope'\n",
    "        ]\n",
    "        # Add feature weights to emphasize important indicators\n",
    "        self.feature_weights = {\n",
    "            'peak_amplitude': 0.3,\n",
    "            'average_y': 0.1,\n",
    "            'distance_difference': 0.1,\n",
    "            'pattern_width': 0.15,\n",
    "            'area_under_curve': 0.15,\n",
    "            'mean_slope': 0.1,\n",
    "            'max_slope': 0.1\n",
    "        }\n",
    "        self.cluster_risks = None\n",
    "\n",
    "    def fit(self, combined_summary):\n",
    "        \"\"\"Train the analyzer on historical data\"\"\"\n",
    "        try:\n",
    "            # Extract features\n",
    "            X = combined_summary[self.feature_cols]\n",
    "            \n",
    "            # Scale features\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            \n",
    "            # Apply feature weights\n",
    "            X_weighted = X_scaled * np.array([self.feature_weights[col] for col in self.feature_cols])\n",
    "            \n",
    "            # Fit KMeans\n",
    "            self.kmeans.fit(X_weighted)\n",
    "            \n",
    "            # Calculate cluster risk scores\n",
    "            self.cluster_risks = self._calculate_cluster_risks(X_weighted)\n",
    "            \n",
    "            return self\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during fitting: {str(e)}\")\n",
    "            # Set default risk scores if fitting fails\n",
    "            self.cluster_risks = np.linspace(0, 1, self.n_clusters)\n",
    "            return self\n",
    "\n",
    "    def _calculate_cluster_risks(self, X_scaled):\n",
    "        \"\"\"Calculate risk scores for each cluster using weighted features\"\"\"\n",
    "        cluster_centers = self.kmeans.cluster_centers_\n",
    "        feature_importance = np.array([self.feature_weights[col] for col in self.feature_cols])\n",
    "        \n",
    "        # Calculate weighted distances from ideal state\n",
    "        weighted_distances = np.sum(np.abs(cluster_centers) * feature_importance, axis=1)\n",
    "        \n",
    "        # Normalize to 0-1 range but maintain sensitivity\n",
    "        risks = (weighted_distances - weighted_distances.min()) / (weighted_distances.max() - weighted_distances.min())\n",
    "        \n",
    "        # Apply sigmoid transformation to spread out the middle range\n",
    "        risks = 1 / (1 + np.exp(-5 * (risks - 0.5)))\n",
    "        \n",
    "        return risks\n",
    "\n",
    "    def predict_risk(self, features_df):\n",
    "        \"\"\"Enhanced risk prediction with temporal smoothing\"\"\"\n",
    "        try:\n",
    "            X = features_df[self.feature_cols]\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            \n",
    "            # Apply feature weights\n",
    "            X_weighted = X_scaled * np.array([self.feature_weights[col] for col in self.feature_cols])\n",
    "            \n",
    "            clusters = self.kmeans.predict(X_weighted)\n",
    "            \n",
    "            # Calculate base risk scores\n",
    "            risk_scores = []\n",
    "            for i, (cluster, point) in enumerate(zip(clusters, X_weighted)):\n",
    "                # Get base risk from cluster\n",
    "                base_risk = self.cluster_risks[cluster]\n",
    "                \n",
    "                # Calculate point-specific adjustments\n",
    "                feature_deviations = np.abs(point - self.kmeans.cluster_centers_[cluster])\n",
    "                weighted_deviations = np.sum(feature_deviations)\n",
    "                \n",
    "                # Calculate dynamic adjustment factor\n",
    "                adjustment = self._sigmoid(weighted_deviations - 0.5) * 0.2  # Max 20% adjustment\n",
    "                \n",
    "                # Apply temporal smoothing if we have previous scores\n",
    "                risk = base_risk + adjustment\n",
    "                if i > 0:\n",
    "                    risk = 0.7 * risk + 0.3 * risk_scores[-1]  # Exponential smoothing\n",
    "                \n",
    "                risk_scores.append(min(max(risk, 0), 1))  # Ensure 0-1 bounds\n",
    "            \n",
    "            return np.array(risk_scores)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during risk prediction: {str(e)}\")\n",
    "            return self._fallback_risk_calculation(features_df)\n",
    "    \n",
    "    def _fallback_risk_calculation(self, features_df):\n",
    "        \"\"\"More nuanced fallback risk calculation\"\"\"\n",
    "        risks = []\n",
    "        for _, row in features_df.iterrows():\n",
    "            # Combine multiple features for risk assessment\n",
    "            peak_risk = min(row['peak_amplitude'] / 10, 1)\n",
    "            width_risk = min(row['pattern_width'] / 20, 1)\n",
    "            slope_risk = min(row['max_slope'] / 5, 1)\n",
    "            \n",
    "            # Weighted combination\n",
    "            combined_risk = (0.5 * peak_risk + \n",
    "                           0.3 * width_risk + \n",
    "                           0.2 * slope_risk)\n",
    "            risks.append(combined_risk)\n",
    "        \n",
    "        return np.array(risks)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Sigmoid function for smooth risk adjustments\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aeab0ad-3a48-42d1-8035-458dec5bb262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_file(input_file, output_dir, channel, date_str, analyzer=None, identifier=None):\n",
    "    \"\"\"\n",
    "    Process a single file and save results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to input CSV file\n",
    "    output_dir : str\n",
    "        Path to output directory\n",
    "    channel : str\n",
    "        Channel to process\n",
    "    date_str : str\n",
    "        Date string for the file\n",
    "    analyzer : CTAnalyzer, optional\n",
    "        Analyzer for risk scoring\n",
    "    identifier : IslandIdentifier, optional\n",
    "        Island identifier for consistent IDs across dates\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read and process data\n",
    "        data = pd.read_csv(input_file)\n",
    "        channel_data = data[data['Channel'] == channel].copy()\n",
    "        \n",
    "        if channel_data.empty:\n",
    "            print(f\"No data found for channel {channel} in file {input_file}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract features\n",
    "        extractor = CTFeatureExtractor(channel_data)\n",
    "        features = extractor.extract_all_features()\n",
    "        \n",
    "        # Create summary DataFrame with location bounds\n",
    "        summary_data = []\n",
    "        for island_id, feature_dict in features.items():\n",
    "            # Extract non-array features including start and end locations\n",
    "            feature_summary = {k: v for k, v in feature_dict.items() \n",
    "                             if not isinstance(v, np.ndarray) and \n",
    "                             k in ['island_id', 'start_location', 'end_location',\n",
    "                                  'peak_amplitude', 'average_y', 'pattern_width',\n",
    "                                  'center_location', 'mean_slope', 'max_slope',\n",
    "                                  'area_under_curve', 'distance_difference']}\n",
    "            summary_data.append(feature_summary)\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Add date column\n",
    "        summary_df['date'] = date_str\n",
    "        \n",
    "        # Apply island identification if provided\n",
    "        if identifier is not None:\n",
    "            # Register islands and get consistent global IDs\n",
    "            summary_df = identifier.register_islands(summary_df, date_str)\n",
    "            \n",
    "            # Update features with global IDs\n",
    "            for idx, row in summary_df.iterrows():\n",
    "                old_id = row['island_id']\n",
    "                features[old_id]['global_id'] = row['global_id']\n",
    "                features[old_id]['match_confidence'] = row['match_confidence']\n",
    "        \n",
    "        # Calculate risk scores if analyzer is provided\n",
    "        if analyzer is not None:\n",
    "            summary_df['risk_score'] = analyzer.predict_risk(summary_df)\n",
    "            \n",
    "            # Update features with risk scores\n",
    "            for idx, row in summary_df.iterrows():\n",
    "                features[row['island_id']]['risk_score'] = row['risk_score']\n",
    "        \n",
    "        # Save summary data\n",
    "        output_file = os.path.join(output_dir, 'summaries', f'{date_str}_summary.csv')\n",
    "        summary_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Generate and save plot\n",
    "        if hasattr(extractor, 'plot_all_islands'):\n",
    "            fig = extractor.plot_all_islands(date_str)\n",
    "            plot_file = os.path.join(output_dir, 'plots', f'{date_str}_analysis.html')\n",
    "            fig.write_html(plot_file)\n",
    "        \n",
    "        return summary_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {input_file}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4d147a-45f8-4ad6-9986-98c9354044bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class IslandSequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for handling multiple island sequences\"\"\"\n",
    "    def __init__(self, data, sequence_length=5):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scalers = {}\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # Group data by global_id\n",
    "        grouped = data.groupby('global_id')\n",
    "        \n",
    "        # Features to use for prediction\n",
    "        self.features = [\n",
    "            'peak_amplitude', 'pattern_width', 'area_under_curve',\n",
    "            'mean_slope', 'max_slope', 'risk_score'\n",
    "        ]\n",
    "        \n",
    "        # Process each island's data\n",
    "        for island_id, island_data in grouped:\n",
    "            if len(island_data) >= sequence_length + 1:  # Need at least sequence_length + 1 points\n",
    "                # Sort by date\n",
    "                island_data = island_data.sort_values('date')\n",
    "                \n",
    "                # Scale features for this island\n",
    "                scaler = MinMaxScaler()\n",
    "                scaled_data = scaler.fit_transform(island_data[self.features])\n",
    "                self.scalers[island_id] = scaler\n",
    "                \n",
    "                # Create sequences\n",
    "                for i in range(len(scaled_data) - sequence_length):\n",
    "                    self.sequences.append(scaled_data[i:i+sequence_length])\n",
    "                    self.targets.append(scaled_data[i+sequence_length, -1])  # risk_score is last feature\n",
    "        \n",
    "        self.sequences = torch.FloatTensor(np.array(self.sequences))\n",
    "        self.targets = torch.FloatTensor(np.array(self.targets))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    \"\"\"LSTM model for risk trajectory prediction\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Use only the last output for prediction\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Predict risk score\n",
    "        prediction = self.fc(last_output)\n",
    "        return prediction\n",
    "\n",
    "class RiskTrajectoryPredictor:\n",
    "    \"\"\"Manager class for risk trajectory prediction\"\"\"\n",
    "    def __init__(self, sequence_length=5, hidden_size=64, num_layers=2, learning_rate=0.001):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dataset = None\n",
    "        self.model = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def prepare_data(self, combined_summary_df):\n",
    "        \"\"\"Prepare dataset from combined summary DataFrame\"\"\"\n",
    "        self.dataset = IslandSequenceDataset(\n",
    "            combined_summary_df, \n",
    "            sequence_length=self.sequence_length\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        input_size = len(self.dataset.features)\n",
    "        self.model = LSTMPredictor(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers\n",
    "        ).to(self.device)\n",
    "\n",
    "    def train(self, num_epochs=50, batch_size=32):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        if self.dataset is None or self.model is None:\n",
    "            raise ValueError(\"Call prepare_data first\")\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for sequences, targets in dataloader:\n",
    "                sequences = sequences.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(sequences)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "    def predict_trajectory(self, island_data, future_steps=90):\n",
    "        \"\"\"\n",
    "        Predict future risk trajectory for an island\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        island_data : pd.DataFrame\n",
    "            Historical data for the island\n",
    "        future_steps : int, optional (default=90)\n",
    "            Number of days to predict into the future\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Array of predicted risk scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Sort by date and get recent sequence\n",
    "            island_data = island_data.sort_values('date')\n",
    "            recent_data = island_data.iloc[-self.sequence_length:]\n",
    "            \n",
    "            # Ensure numerical values\n",
    "            for col in self.dataset.features:\n",
    "                if col in recent_data.columns:\n",
    "                    recent_data[col] = pd.to_numeric(recent_data[col], errors='coerce')\n",
    "            \n",
    "            # Scale the data\n",
    "            scaler = MinMaxScaler()\n",
    "            feature_data = recent_data[self.dataset.features].astype(float)\n",
    "            scaled_data = scaler.fit_transform(feature_data)\n",
    "            \n",
    "            # Convert to tensor\n",
    "            sequence = torch.FloatTensor(scaled_data).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            predictions = []\n",
    "            with torch.no_grad():\n",
    "                current_sequence = sequence\n",
    "                \n",
    "                for _ in range(future_steps):\n",
    "                    # Predict next risk score\n",
    "                    output = self.model(current_sequence)\n",
    "                    pred_risk = float(output.item())  # Convert to float explicitly\n",
    "                    predictions.append(pred_risk)\n",
    "                    \n",
    "                    # Update sequence for next prediction\n",
    "                    new_sequence = current_sequence.clone()\n",
    "                    new_sequence = new_sequence[:, 1:, :]  # Remove oldest timestep\n",
    "                    \n",
    "                    # Create new feature vector using last known values and predicted risk\n",
    "                    new_features = new_sequence[:, -1, :].clone()\n",
    "                    new_features[:, -1] = pred_risk  # Update risk score\n",
    "                    \n",
    "                    # Add new timestep\n",
    "                    new_sequence = torch.cat([\n",
    "                        new_sequence,\n",
    "                        new_features.unsqueeze(1)\n",
    "                    ], dim=1)\n",
    "                    \n",
    "                    current_sequence = new_sequence\n",
    "            \n",
    "            # Convert predictions to numpy array\n",
    "            predictions = np.array(predictions, dtype=np.float32)\n",
    "            \n",
    "            # Inverse transform predictions\n",
    "            predictions_2d = predictions.reshape(-1, 1)\n",
    "            padding = np.zeros((len(predictions), len(self.dataset.features)-1))\n",
    "            padded_predictions = np.hstack([padding, predictions_2d])\n",
    "            inverse_transformed = scaler.inverse_transform(padded_predictions)[:, -1]\n",
    "            \n",
    "            # Ensure predictions are within valid range [0, 1] and are float type\n",
    "            return np.clip(inverse_transformed, 0, 1).astype(np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction: {str(e)}\")\n",
    "            return np.array([], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b591194-e1f5-4dd5-ba51-3f0c088ba070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TemporalEvolutionAnalyzer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.features_to_track = [\n",
    "            'peak_amplitude',\n",
    "            'pattern_width',\n",
    "            'area_under_curve',\n",
    "            'mean_slope',\n",
    "            'max_slope',\n",
    "            'risk_score'\n",
    "        ]\n",
    "        self.risk_predictor = None\n",
    "\n",
    "    # New LSTM addition\n",
    "    def initialize_risk_predictor(self, combined_summary_df):\n",
    "        \"\"\"Initialize and train the LSTM risk predictor\"\"\"\n",
    "        self.risk_predictor = RiskTrajectoryPredictor()\n",
    "        self.risk_predictor.prepare_data(combined_summary_df)\n",
    "        self.risk_predictor.train()\n",
    "\n",
    "    def analyze_island_evolution(self, island_data):\n",
    "        \"\"\"Analyze temporal evolution of a single island\"\"\"\n",
    "        evolution_metrics = {}\n",
    "        island_data = island_data.sort_values('date')\n",
    "        \n",
    "        # Calculate days since first observation\n",
    "        island_data['days_since_start'] = (\n",
    "            pd.to_datetime(island_data['date']) - \n",
    "            pd.to_datetime(island_data['date']).min()\n",
    "        ).dt.total_seconds() / (24 * 3600)\n",
    "        \n",
    "        evolution_metrics = {}\n",
    "        \n",
    "        for feature in self.features_to_track:\n",
    "            values = island_data[feature].values\n",
    "            days = island_data['days_since_start'].values\n",
    "            \n",
    "            # Fit exponential growth model\n",
    "            try:\n",
    "                # log(y) = log(a) + bx\n",
    "                # y = ae^(bx)\n",
    "                log_values = np.log(values)\n",
    "                slope, intercept, r_value, _, _ = stats.linregress(days, log_values)\n",
    "                \n",
    "                evolution_metrics[f'{feature}_growth_rate'] = slope\n",
    "                evolution_metrics[f'{feature}_initial_value'] = np.exp(intercept)\n",
    "                evolution_metrics[f'{feature}_r_squared'] = r_value**2\n",
    "                \n",
    "                # Project time to critical (assuming critical is 2x current max)\n",
    "                if slope > 0:  # Only if growing\n",
    "                    critical_value = 2 * values.max()\n",
    "                    time_to_critical = (np.log(critical_value) - intercept) / slope\n",
    "                    evolution_metrics[f'{feature}_days_to_critical'] = time_to_critical\n",
    "                else:\n",
    "                    evolution_metrics[f'{feature}_days_to_critical'] = np.inf\n",
    "                \n",
    "            except:\n",
    "                evolution_metrics[f'{feature}_growth_rate'] = 0\n",
    "                evolution_metrics[f'{feature}_initial_value'] = values[0]\n",
    "                evolution_metrics[f'{feature}_r_squared'] = 0\n",
    "                evolution_metrics[f'{feature}_days_to_critical'] = np.inf\n",
    "\n",
    "                    # Add LSTM predictions if predictor is initialized\n",
    "        if self.risk_predictor is not None and len(island_data) >= self.risk_predictor.sequence_length:\n",
    "            future_predictions = self.risk_predictor.predict_trajectory(island_data)\n",
    "            evolution_metrics['predicted_risk_trajectory'] = future_predictions.tolist()\n",
    "            evolution_metrics['predicted_max_risk'] = max(future_predictions)\n",
    "            evolution_metrics['predicted_risk_trend'] = (\n",
    "                future_predictions[-1] - future_predictions[0]\n",
    "            ) / len(future_predictions)\n",
    "        \n",
    "        return evolution_metrics\n",
    "    \n",
    "    \n",
    "\n",
    "    def plot_spectral_heatmap(self, data, feature, output_dir):\n",
    "        \"\"\"Create a heatmap showing feature evolution with Island IDs\"\"\"\n",
    "        # Prepare data\n",
    "        pivot_data = data.pivot(\n",
    "            index='date',\n",
    "            columns='global_id',\n",
    "            values=feature\n",
    "        )\n",
    "        \n",
    "        # Get location information for each island\n",
    "        island_info = data.groupby('global_id').agg({\n",
    "            'start_location': 'mean',\n",
    "            'end_location': 'mean'\n",
    "        }).sort_values('start_location')\n",
    "        \n",
    "        # Sort islands by location\n",
    "        pivot_data = pivot_data.reindex(columns=island_info.index)\n",
    "        \n",
    "        # Create x-axis labels with Island IDs and locations\n",
    "        x_labels = [\n",
    "            f\"IS:{id}<br>{start:.0f}-{end:.0f}m\" \n",
    "            for id, (start, end) in zip(\n",
    "                island_info.index, \n",
    "                island_info[['start_location', 'end_location']].values\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=pivot_data.values,\n",
    "            x=x_labels,\n",
    "            y=pivot_data.index,\n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(title=feature)\n",
    "        ))\n",
    "        \n",
    "        # Update layout for visual adjustments\n",
    "        fig.update_layout(\n",
    "            title=f'Temporal Evolution of {feature} Across Track',\n",
    "            xaxis_title='Island ID and Location',\n",
    "            yaxis_title='Date',\n",
    "            height=800,\n",
    "            xaxis=dict(\n",
    "                tickangle=90,\n",
    "                tickmode='array',\n",
    "                ticktext=x_labels,\n",
    "                tickvals=list(range(len(x_labels))),\n",
    "                showgrid=False  # Disable x-axis gridlines\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                showgrid=False  # Disable y-axis gridlines\n",
    "            ),\n",
    "            paper_bgcolor='black',  # Set the overall background color\n",
    "            plot_bgcolor='black',  # Set the plot's background color\n",
    "            font=dict(color='white')  # Adjust font color for visibility on black\n",
    "        )\n",
    "        \n",
    "        fig.write_html(os.path.join(output_dir, f'spectral_evolution_{feature}.html'))\n",
    "\n",
    "    def plot_island_evolution(self, island_data, global_id, output_dir):\n",
    "        \"\"\"Create evolution plot with comprehensive trend analysis and predictions\"\"\"\n",
    "        try:\n",
    "            island_data = island_data.copy()  # Make a copy to prevent modifications\n",
    "            island_data = island_data.sort_values('date')\n",
    "            dates = pd.to_datetime(island_data['date'])\n",
    "            \n",
    "            # Create subplots for each feature\n",
    "            fig = make_subplots(\n",
    "                rows=len(self.features_to_track), \n",
    "                cols=1,\n",
    "                subplot_titles=[f'{feature.replace(\"_\", \" \").title()} Evolution' \n",
    "                            for feature in self.features_to_track],\n",
    "                vertical_spacing=0.08\n",
    "            )\n",
    "            \n",
    "            start_loc = float(island_data['start_location'].mean())  # Explicitly convert to float\n",
    "            end_loc = float(island_data['end_location'].mean())  # Explicitly convert to float\n",
    "            \n",
    "            for i, feature in enumerate(self.features_to_track, 1):\n",
    "                try:\n",
    "                    # Convert values to float explicitly\n",
    "                    values = island_data[feature].astype(float).values\n",
    "                    \n",
    "                    # Plot actual values\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=dates,\n",
    "                            y=values,\n",
    "                            mode='markers+lines',\n",
    "                            name=f'Actual {feature.replace(\"_\", \" \").title()}',\n",
    "                            marker=dict(size=8),\n",
    "                            line=dict(width=2)\n",
    "                        ),\n",
    "                        row=i, col=1\n",
    "                    )\n",
    "                    \n",
    "                    # Linear regression for trend analysis\n",
    "                    days = (dates - dates.min()).dt.total_seconds() / (24 * 3600)\n",
    "                    days = days.astype(float)\n",
    "                    \n",
    "                    slope, intercept, r_value, p_value, std_err = stats.linregress(days, values)\n",
    "                    r_squared = r_value ** 2\n",
    "                    \n",
    "                    # Generate trend line\n",
    "                    future_days = np.linspace(0, days.max() * 2, 100)\n",
    "                    trend_dates = dates.min() + pd.Timedelta(days=float(future_days.max()))\n",
    "                    trend_values = slope * future_days + intercept\n",
    "                    \n",
    "                    # Add LSTM predictions specifically for risk_score\n",
    "                    if (feature == 'risk_score' and \n",
    "                        self.risk_predictor is not None and \n",
    "                        len(island_data) >= self.risk_predictor.sequence_length):\n",
    "                        try:\n",
    "                            predictions = self.risk_predictor.predict_trajectory(island_data)\n",
    "                            if len(predictions) > 0:\n",
    "                                # Generate future dates for predictions\n",
    "                                last_date = dates.max()\n",
    "                                future_dates = pd.date_range(\n",
    "                                    start=last_date + pd.Timedelta(days=1),\n",
    "                                    periods=len(predictions),\n",
    "                                    freq='D'\n",
    "                                )\n",
    "                                \n",
    "                                # Add prediction line\n",
    "                                fig.add_trace(\n",
    "                                    go.Scatter(\n",
    "                                        x=future_dates,\n",
    "                                        y=predictions,\n",
    "                                        mode='lines',\n",
    "                                        name='LSTM Predictions',\n",
    "                                        line=dict(\n",
    "                                            dash='dash',\n",
    "                                            color='red',\n",
    "                                            width=2\n",
    "                                        )\n",
    "                                    ),\n",
    "                                    row=i, col=1\n",
    "                                )\n",
    "                                \n",
    "                                # Add uncertainty ribbon if available\n",
    "                                if hasattr(self.risk_predictor, 'get_prediction_uncertainty'):\n",
    "                                    lower_bound, upper_bound = self.risk_predictor.get_prediction_uncertainty(predictions)\n",
    "                                    fig.add_trace(\n",
    "                                        go.Scatter(\n",
    "                                            x=future_dates,\n",
    "                                            y=upper_bound,\n",
    "                                            mode='lines',\n",
    "                                            line=dict(width=0),\n",
    "                                            showlegend=False\n",
    "                                        ),\n",
    "                                        row=i, col=1\n",
    "                                    )\n",
    "                                    fig.add_trace(\n",
    "                                        go.Scatter(\n",
    "                                            x=future_dates,\n",
    "                                            y=lower_bound,\n",
    "                                            mode='lines',\n",
    "                                            line=dict(width=0),\n",
    "                                            fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "                                            fill='tonexty',\n",
    "                                            name='95% Confidence'\n",
    "                                        ),\n",
    "                                        row=i, col=1\n",
    "                                    )\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error adding predictions for island {global_id}: {str(e)}\")\n",
    "                    \n",
    "                    # Update axes labels\n",
    "                    fig.update_xaxes(\n",
    "                        title_text=\"Date\",\n",
    "                        tickformat=\"%Y-%m-%d\",\n",
    "                        tickangle=45,\n",
    "                        row=i, col=1\n",
    "                    )\n",
    "                    fig.update_yaxes(\n",
    "                        title_text=feature.replace(\"_\", \" \").title(),\n",
    "                        row=i, col=1\n",
    "                    )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {feature} for island {global_id}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                height=250 * len(self.features_to_track),\n",
    "                title=f'Evolution of Island {global_id}<br>(Location: {start_loc:.1f}m - {end_loc:.1f}m)',\n",
    "                showlegend=True,\n",
    "                legend=dict(\n",
    "                    yanchor=\"top\",\n",
    "                    y=0.99,\n",
    "                    xanchor=\"left\",\n",
    "                    x=1.05\n",
    "                ),\n",
    "                margin=dict(r=250, b=50)\n",
    "            )\n",
    "            \n",
    "            # Save plot\n",
    "            try:\n",
    "                fig.write_html(os.path.join(output_dir, f'evolution_island_{global_id}.html'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving plot for island {global_id}: {str(e)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in plot_island_evolution for island {global_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e1f4f1-2181-42c9-9336-9658a7e1a656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_temporal_evolution(combined_summary_file, output_dir):\n",
    "    \"\"\"Perform temporal evolution analysis\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read data\n",
    "    print(\"Reading data...\")\n",
    "    data = pd.read_csv(combined_summary_file)\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = TemporalEvolutionAnalyzer()\n",
    "    \n",
    "    # Analyze each island\n",
    "    print(\"Analyzing island evolution patterns...\")\n",
    "    evolution_results = []\n",
    "    \n",
    "    for global_id in data['global_id'].unique():\n",
    "        island_data = data[data['global_id'] == global_id]\n",
    "        if len(island_data) >= 3:  # Minimum points for trend analysis\n",
    "            metrics = analyzer.analyze_island_evolution(island_data)\n",
    "            metrics['global_id'] = global_id\n",
    "            \n",
    "            # Add location information\n",
    "            metrics['start_location'] = island_data['start_location'].mean()\n",
    "            metrics['end_location'] = island_data['end_location'].mean()\n",
    "            evolution_results.append(metrics)\n",
    "            \n",
    "            # Create individual evolution plot\n",
    "            analyzer.plot_island_evolution(island_data, global_id, output_dir)\n",
    "    \n",
    "    evolution_df = pd.DataFrame(evolution_results)\n",
    "    \n",
    "    # Create spectral evolution plots\n",
    "    print(\"Creating spectral evolution plots...\")\n",
    "    for feature in analyzer.features_to_track:\n",
    "        analyzer.plot_spectral_heatmap(data, feature, output_dir)\n",
    "    \n",
    "    # Save evolution metrics\n",
    "    evolution_df.to_csv(os.path.join(output_dir, 'evolution_metrics.csv'), index=False)\n",
    "    \n",
    "    # Create summary visualizations\n",
    "    create_summary_visualizations(evolution_df, output_dir)\n",
    "    \n",
    "    return evolution_df\n",
    "\n",
    "\n",
    "def create_summary_visualizations(evolution_df, output_dir):\n",
    "    \"\"\"Create summary visualizations with fixed colorbar\"\"\"\n",
    "    features = [\n",
    "        'peak_amplitude', 'pattern_width', 'area_under_curve',\n",
    "        'mean_slope', 'max_slope', 'risk_score'\n",
    "    ]\n",
    "    \n",
    "    # Create one main colorbar for R² values\n",
    "    colorbar_settings = dict(\n",
    "        title=dict(\n",
    "            text='R²',\n",
    "            side='right'\n",
    "        ),\n",
    "        tickformat='.2f',\n",
    "        len=0.5,  # Length of colorbar\n",
    "        yanchor='middle',\n",
    "        y=0.5,    # Center vertically\n",
    "        xanchor='left',\n",
    "        x=1.05    # Position to the right of plots\n",
    "    )\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=[\n",
    "            f\"{feature.replace('_', ' ').title()} Growth Rate vs Initial Value\"\n",
    "            for feature in features\n",
    "        ],\n",
    "        vertical_spacing=0.2,\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    row = 1\n",
    "    col = 1\n",
    "    for feature in features:\n",
    "        # Create scatter plot\n",
    "        scatter = go.Scatter(\n",
    "            x=evolution_df[f'{feature}_initial_value'],\n",
    "            y=evolution_df[f'{feature}_growth_rate'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=evolution_df[f'{feature}_r_squared'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=(col == 3 and row == 1),  # Show colorbar only once\n",
    "                colorbar=colorbar_settings if (col == 3 and row == 1) else None\n",
    "            ),\n",
    "            text=[\n",
    "                f\"Location: {row['start_location']:.1f}m - {row['end_location']:.1f}m<br>\" +\n",
    "                f\"Global ID: {row['global_id']}<br>\" +\n",
    "                f\"R²: {row[f'{feature}_r_squared']:.3f}\"\n",
    "                for _, row in evolution_df.iterrows()\n",
    "            ],\n",
    "            hovertemplate=\"Initial Value: %{x:.2f}<br>\" +\n",
    "                         \"Growth Rate: %{y:.2e}/day<br>\" +\n",
    "                         \"%{text}<br>\" +\n",
    "                         \"<extra></extra>\"\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(scatter, row=row, col=col)\n",
    "        \n",
    "        # Update axes labels\n",
    "        fig.update_xaxes(\n",
    "            title_text=f\"Initial {feature.replace('_', ' ').title()}\",\n",
    "            row=row, col=col\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            title_text=\"Growth Rate (per day)\",\n",
    "            row=row, col=col\n",
    "        )\n",
    "        \n",
    "        col += 1\n",
    "        if col > 3:\n",
    "            col = 1\n",
    "            row += 1\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        width=1500,\n",
    "        title='Growth Rate Analysis of Track Features',\n",
    "        showlegend=False,\n",
    "        template='plotly_white',\n",
    "        margin=dict(r=150)  # Make room for colorbar\n",
    "    )\n",
    "    \n",
    "    fig.write_html(os.path.join(output_dir, 'growth_rate_analysis.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257e78b8-6ee0-408e-925b-9e5d1682073e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict_threshold_crossing(risk_scores, dates, threshold, max_forecast_days=365):\n",
    "    \"\"\"\n",
    "    Predict when risk score will cross a threshold based on historical trend.\n",
    "    \n",
    "    Parameters:\n",
    "        risk_scores: array of historical risk scores\n",
    "        dates: array of corresponding dates\n",
    "        threshold: risk score threshold to predict crossing\n",
    "        max_forecast_days: maximum number of days to forecast into future\n",
    "    \n",
    "    Returns:\n",
    "        predicted_date: datetime or None if threshold won't be crossed within max_forecast_days\n",
    "    \"\"\"\n",
    "    if len(risk_scores) < 2:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Convert dates to numerical days since first observation\n",
    "        dates_numeric = np.array([(pd.to_datetime(d) - pd.to_datetime(dates[0])).days \n",
    "                                for d in dates])\n",
    "        \n",
    "        # Fit polynomial regression (degree 2 for better trend capture)\n",
    "        coeffs = np.polyfit(dates_numeric, risk_scores, deg=2)\n",
    "        poly = np.poly1d(coeffs)\n",
    "        \n",
    "        # Find root of polynomial - threshold equation\n",
    "        # We're looking for future dates only\n",
    "        future_days = np.arange(dates_numeric[-1] + 1, \n",
    "                              dates_numeric[-1] + max_forecast_days)\n",
    "        \n",
    "        future_risks = poly(future_days)\n",
    "        \n",
    "        # Find first crossing of threshold\n",
    "        crossing_idx = np.where(future_risks >= threshold)[0]\n",
    "        \n",
    "        if len(crossing_idx) > 0:\n",
    "            days_until_threshold = future_days[crossing_idx[0]] - dates_numeric[-1]\n",
    "            predicted_date = pd.to_datetime(dates[-1]) + pd.Timedelta(days=days_until_threshold)\n",
    "            return predicted_date\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in threshold prediction: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401593a9-046a-4722-b0c7-a74fe4c9f585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_threshold_crossings(combined_summary_df, evolution_df, output_dir, thresholds=[0.9, 1.0], prediction_days=180):\n",
    "    \"\"\"\n",
    "    Analyze when each island is predicted to cross specific risk thresholds\n",
    "    \"\"\"\n",
    "    # Initialize LSTM predictor if not already done\n",
    "    risk_predictor = RiskTrajectoryPredictor()\n",
    "    risk_predictor.prepare_data(combined_summary_df)\n",
    "    risk_predictor.train()\n",
    "    \n",
    "    threshold_results = []\n",
    "    \n",
    "    for _, row in evolution_df.iterrows():\n",
    "        global_id = row['global_id']\n",
    "        \n",
    "        # Get historical data for this island\n",
    "        island_data = combined_summary_df[\n",
    "            combined_summary_df['global_id'] == global_id\n",
    "        ].sort_values('date')\n",
    "        \n",
    "        if len(island_data) >= risk_predictor.sequence_length:\n",
    "            # Get the last known date and risk score\n",
    "            last_date = pd.to_datetime(island_data['date'].max())\n",
    "            last_risk = island_data['risk_score'].iloc[-1]\n",
    "            \n",
    "            # Get start and end locations from the island_data\n",
    "            start_location = island_data['start_location'].mean()\n",
    "            end_location = island_data['end_location'].mean()\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = risk_predictor.predict_trajectory(island_data, future_steps=prediction_days)\n",
    "            \n",
    "            # Create future dates\n",
    "            future_dates = pd.date_range(\n",
    "                start=last_date,\n",
    "                periods=len(predictions) + 1,\n",
    "                freq='D'\n",
    "            )[1:]\n",
    "            \n",
    "            # Initialize result dictionary\n",
    "            result = {\n",
    "                'global_id': global_id,\n",
    "                'start_location': start_location,\n",
    "                'end_location': end_location,\n",
    "                'last_measurement_date': last_date.strftime('%Y-%m-%d'),\n",
    "                'current_risk_score': last_risk\n",
    "            }\n",
    "            \n",
    "            # Find crossing dates for each threshold\n",
    "            for threshold in thresholds:\n",
    "                # Check if already crossed\n",
    "                if last_risk >= threshold:\n",
    "                    result[f'threshold_{threshold}_crossed'] = True\n",
    "                    result[f'threshold_{threshold}_crossing_date'] = 'Already Crossed'\n",
    "                    result[f'days_until_{threshold}'] = 0\n",
    "                else:\n",
    "                    # Find first crossing of threshold\n",
    "                    crossing_indices = np.where(predictions >= threshold)[0]\n",
    "                    \n",
    "                    if len(crossing_indices) > 0:\n",
    "                        crossing_idx = crossing_indices[0]\n",
    "                        crossing_date = future_dates[crossing_idx]\n",
    "                        \n",
    "                        result[f'threshold_{threshold}_crossed'] = True\n",
    "                        result[f'threshold_{threshold}_crossing_date'] = crossing_date.strftime('%Y-%m-%d')\n",
    "                        result[f'days_until_{threshold}'] = (crossing_date - last_date).days\n",
    "                    else:\n",
    "                        result[f'threshold_{threshold}_crossed'] = False\n",
    "                        result[f'threshold_{threshold}_crossing_date'] = 'Not Within Prediction Window'\n",
    "                        result[f'days_until_{threshold}'] = None\n",
    "            \n",
    "            # Calculate velocity and acceleration of risk increase\n",
    "            if len(predictions) > 1:\n",
    "                risk_velocity = (predictions[1] - predictions[0])  # risk/day\n",
    "                result['risk_velocity'] = risk_velocity\n",
    "                \n",
    "                if len(predictions) > 2:\n",
    "                    risk_acceleration = (predictions[2] - 2*predictions[1] + predictions[0])  # risk/day²\n",
    "                    result['risk_acceleration'] = risk_acceleration\n",
    "                else:\n",
    "                    result['risk_acceleration'] = 0\n",
    "            else:\n",
    "                result['risk_velocity'] = 0\n",
    "                result['risk_acceleration'] = 0\n",
    "            \n",
    "            threshold_results.append(result)\n",
    "    \n",
    "    # Create DataFrame and sort by urgency\n",
    "    results_df = pd.DataFrame(threshold_results)\n",
    "    \n",
    "    # Sort by days until first threshold crossing (excluding already crossed)\n",
    "    for threshold in thresholds:\n",
    "        mask = results_df[f'days_until_{threshold}'].notna()\n",
    "        results_df.loc[mask, 'min_days_to_threshold'] = results_df.loc[mask, f'days_until_{threshold}']\n",
    "    \n",
    "    results_df['min_days_to_threshold'] = results_df['min_days_to_threshold'].fillna(float('inf'))\n",
    "    results_df = results_df.sort_values('min_days_to_threshold')\n",
    "    \n",
    "    # Save results\n",
    "    output_file = os.path.join(output_dir, 'threshold_crossing_predictions.csv')\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary_stats = {\n",
    "        'total_islands': len(results_df),\n",
    "        'islands_crossing_thresholds': {\n",
    "            str(threshold): {\n",
    "                'already_crossed': len(results_df[results_df[f'threshold_{threshold}_crossing_date'] == 'Already Crossed']),\n",
    "                'will_cross': len(results_df[results_df[f'threshold_{threshold}_crossed'] & \n",
    "                                          (results_df[f'threshold_{threshold}_crossing_date'] != 'Already Crossed')]),\n",
    "                'wont_cross': len(results_df[~results_df[f'threshold_{threshold}_crossed']])\n",
    "            }\n",
    "            for threshold in thresholds\n",
    "        },\n",
    "        'average_days_to_threshold': {\n",
    "            str(threshold): results_df[results_df[f'days_until_{threshold}'].notna()][f'days_until_{threshold}'].mean()\n",
    "            for threshold in thresholds\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save summary statistics\n",
    "    with open(os.path.join(output_dir, 'threshold_crossing_summary.json'), 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=4)\n",
    "    \n",
    "    return results_df, summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "546e60c7-4d74-4ac1-8008-481972fd1cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'CHANNEL': '18_top_right',\n",
    "        'INPUT_DIR': '/Workspace/Users/amahmud1@networkrail.co.uk/CT/CT_processed_data/',\n",
    "        'OUTPUT_BASE_DIR': '/Workspace/Users/amahmud1@networkrail.co.uk/CT',\n",
    "        'PHASE1_DIR': '/Workspace/Users/amahmud1@networkrail.co.uk/CT/CT_phase1',\n",
    "        'PHASE3_DIR': '/Workspace/Users/amahmud1@networkrail.co.uk/CT/CT_phase3',\n",
    "        'ISLAND_DISTANCE_THRESHOLD': 5.0,  # meters\n",
    "        'ISLAND_OVERLAP_THRESHOLD': 0.3,   # 30% overlap required\n",
    "        'NUM_CLUSTERS': 5,                 # for risk analysis\n",
    "        'INTERPOLATION_POINTS': 100,       # for spline fitting\n",
    "        'FEATURE_WEIGHTS': {\n",
    "            'peak_amplitude': 0.3,\n",
    "            'average_y': 0.1,\n",
    "            'distance_difference': 0.1,\n",
    "            'pattern_width': 0.15,\n",
    "            'area_under_curve': 0.15,\n",
    "            'mean_slope': 0.1,\n",
    "            'max_slope': 0.1\n",
    "        },\n",
    "        'RISK_SMOOTHING_FACTOR': 0.7,      # Temporal smoothing factor (0-1)\n",
    "        'RISK_ADJUSTMENT_MAX': 0.2         # Maximum risk adjustment (20%)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n=== Starting C-PRICS Analysis Pipeline ===\")\n",
    "        \n",
    "        # Create all necessary directories\n",
    "        for dir_path in [\n",
    "            os.path.join(CONFIG['PHASE1_DIR'], 'summaries'),\n",
    "            os.path.join(CONFIG['PHASE1_DIR'], 'plots'),\n",
    "            CONFIG['PHASE3_DIR']\n",
    "        ]:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Phase 1: Initial Processing and Risk Analysis\n",
    "        print(\"\\n=== Phase 1: Signal Processing and Risk Analysis ===\")\n",
    "        print(f\"Channel: {CONFIG['CHANNEL']}\")\n",
    "        print(f\"Input Directory: {CONFIG['INPUT_DIR']}\")\n",
    "        \n",
    "        # Get sorted files\n",
    "        sorted_files = get_sorted_files(CONFIG['INPUT_DIR'])\n",
    "        print(f\"\\nFound {len(sorted_files)} files to process\")\n",
    "        \n",
    "        # Initialize identifier\n",
    "        identifier = IslandIdentifier(\n",
    "            distance_threshold=CONFIG['ISLAND_DISTANCE_THRESHOLD'],\n",
    "            overlap_threshold=CONFIG['ISLAND_OVERLAP_THRESHOLD']\n",
    "        )\n",
    "        \n",
    "        # First pass: collect all data for ML training and assign global IDs\n",
    "        print(\"\\nFirst pass: Processing files for ML training and initial island identification...\")\n",
    "        all_summaries = []\n",
    "        for date, filename in sorted_files:\n",
    "            print(f\"Processing {filename}...\")\n",
    "            input_file = os.path.join(CONFIG['INPUT_DIR'], filename)\n",
    "            date_str = date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            summary_df = process_file(\n",
    "                input_file=input_file,\n",
    "                output_dir=CONFIG['PHASE1_DIR'],\n",
    "                channel=CONFIG['CHANNEL'],\n",
    "                date_str=date_str,\n",
    "                identifier=identifier\n",
    "            )\n",
    "            \n",
    "            if summary_df is not None:\n",
    "                all_summaries.append(summary_df)\n",
    "        \n",
    "        # Create and train risk analyzer\n",
    "        print(\"\\nTraining risk analyzer...\")\n",
    "        combined_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "        analyzer = CTAnalyzer(n_clusters=CONFIG['NUM_CLUSTERS'])\n",
    "        analyzer.feature_weights = CONFIG['FEATURE_WEIGHTS']\n",
    "        analyzer.fit(combined_summary)\n",
    "        \n",
    "        # Second pass: process files with risk scoring\n",
    "        print(\"\\nSecond pass: Processing files with risk scoring...\")\n",
    "        all_summaries = []\n",
    "        sorted_files.sort(key=lambda x: x[0])\n",
    "        \n",
    "        for date, filename in sorted_files:\n",
    "            print(f\"Processing {filename}...\")\n",
    "            input_file = os.path.join(CONFIG['INPUT_DIR'], filename)\n",
    "            date_str = date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            summary_df = process_file(\n",
    "                input_file=input_file,\n",
    "                output_dir=CONFIG['PHASE1_DIR'],\n",
    "                channel=CONFIG['CHANNEL'],\n",
    "                date_str=date_str,\n",
    "                analyzer=analyzer,\n",
    "                identifier=identifier\n",
    "            )\n",
    "            \n",
    "            if summary_df is not None:\n",
    "                all_summaries.append(summary_df)\n",
    "        \n",
    "        # Save final combined summary\n",
    "        final_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "        final_summary.sort_values(['date', 'global_id'], inplace=True)\n",
    "        combined_summary_path = os.path.join(CONFIG['PHASE1_DIR'], f'combined_summary_{CONFIG[\"CHANNEL\"]}.csv')\n",
    "        final_summary.to_csv(combined_summary_path, index=False)\n",
    "        \n",
    "        # Save evolution history\n",
    "        history_df = identifier.get_history_summary()\n",
    "        history_df.to_csv(os.path.join(CONFIG['PHASE1_DIR'], 'island_history.csv'), index=False)\n",
    "        \n",
    "        # Create initial evolution summary\n",
    "        print(\"\\nGenerating evolution summary...\")\n",
    "        evolution_summary = []\n",
    "        for global_id in final_summary['global_id'].unique():\n",
    "            island_data = final_summary[final_summary['global_id'] == global_id].sort_values('date')\n",
    "            risk_volatility = island_data['risk_score'].std() if len(island_data) > 1 else 0\n",
    "            \n",
    "            summary = {\n",
    "                'global_id': global_id,\n",
    "                'first_appearance': island_data['date'].min(),\n",
    "                'last_appearance': island_data['date'].max(),\n",
    "                'num_observations': len(island_data),\n",
    "                'avg_start_location': island_data['start_location'].mean(),\n",
    "                'avg_end_location': island_data['end_location'].mean(),\n",
    "                'location_drift': max(\n",
    "                    island_data['start_location'].max() - island_data['start_location'].min(),\n",
    "                    island_data['end_location'].max() - island_data['end_location'].min()\n",
    "                ),\n",
    "                'avg_risk_score': island_data['risk_score'].mean(),\n",
    "                'max_risk_score': island_data['risk_score'].max(),\n",
    "                'risk_score_trend': np.polyfit(\n",
    "                    range(len(island_data)), \n",
    "                    island_data['risk_score'], \n",
    "                    1\n",
    "                )[0] if len(island_data) > 1 else 0,\n",
    "                'risk_volatility': risk_volatility,\n",
    "                'risk_acceleration': np.diff(island_data['risk_score']).mean() if len(island_data) > 2 else 0\n",
    "            }\n",
    "            evolution_summary.append(summary)\n",
    "        \n",
    "        evolution_df = pd.DataFrame(evolution_summary)\n",
    "        evolution_df.to_csv(os.path.join(CONFIG['PHASE1_DIR'], 'island_evolution_summary.csv'), index=False)\n",
    "        \n",
    "        # Phase 3: Temporal Evolution Analysis\n",
    "        print(\"\\n=== Phase 3: Temporal Evolution Analysis ===\")\n",
    "        \n",
    "        # Initialize and train LSTM predictor\n",
    "        print(\"\\nInitializing LSTM predictor...\")\n",
    "        temporal_analyzer = TemporalEvolutionAnalyzer()\n",
    "        temporal_analyzer.initialize_risk_predictor(final_summary)\n",
    "        \n",
    "        # Perform temporal evolution analysis\n",
    "        print(\"\\nPerforming temporal evolution analysis...\")\n",
    "        temporal_evolution_df = analyze_temporal_evolution(combined_summary_path, CONFIG['PHASE3_DIR'])\n",
    "        \n",
    "        # Perform threshold crossing analysis\n",
    "        print(\"\\nAnalyzing threshold crossings...\")\n",
    "        threshold_results, threshold_summary = analyze_threshold_crossings(\n",
    "            combined_summary_df=final_summary,\n",
    "            evolution_df=temporal_evolution_df,\n",
    "            output_dir=CONFIG['PHASE3_DIR']\n",
    "        )\n",
    "        \n",
    "        # Save final metadata\n",
    "        metadata = {\n",
    "            'channel': CONFIG['CHANNEL'],\n",
    "            'files_processed': len(sorted_files),\n",
    "            'total_islands': len(final_summary),\n",
    "            'unique_islands': len(evolution_df),\n",
    "            'number_of_clusters': CONFIG['NUM_CLUSTERS'],\n",
    "            'cluster_risks': analyzer.cluster_risks.tolist(),\n",
    "            'feature_weights': CONFIG['FEATURE_WEIGHTS'],\n",
    "            'risk_smoothing_factor': CONFIG['RISK_SMOOTHING_FACTOR'],\n",
    "            'risk_adjustment_max': CONFIG['RISK_ADJUSTMENT_MAX'],\n",
    "            'date_range': [str(final_summary['date'].min()), str(final_summary['date'].max())],\n",
    "            'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'configuration': CONFIG,\n",
    "            'risk_score_statistics': {\n",
    "                'mean': float(final_summary['risk_score'].mean()),\n",
    "                'std': float(final_summary['risk_score'].std()),\n",
    "                'min': float(final_summary['risk_score'].min()),\n",
    "                'max': float(final_summary['risk_score'].max()),\n",
    "                'volatility': float(final_summary.groupby('global_id')['risk_score'].std().mean())\n",
    "            },\n",
    "            'threshold_analysis': threshold_summary,\n",
    "            'temporal_analysis_completed': True\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(CONFIG['OUTPUT_BASE_DIR'], 'full_analysis_metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        \n",
    "        print(\"\\n=== Analysis Pipeline Complete ===\")\n",
    "        print(f\"Processed {len(sorted_files)} files\")\n",
    "        print(f\"Found {len(evolution_df)} unique islands\")\n",
    "        print(f\"Results saved to {CONFIG['OUTPUT_BASE_DIR']}\")\n",
    "        print(f\"Phase 1 results: {CONFIG['PHASE1_DIR']}\")\n",
    "        print(f\"Phase 3 results: {CONFIG['PHASE3_DIR']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in processing pipeline: {str(e)}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "(Clone_4) Cyclic Tops Classification",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
